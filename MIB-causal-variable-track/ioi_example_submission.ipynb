{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOI Example Submission - DAS Training for Attention Heads\n",
    "\n",
    "This notebook demonstrates how to train DAS (Direct Attribution with Subspace) on attention heads for the IOI (Indirect Object Identification) task using a Gemma model.\n",
    "\n",
    "## Overview\n",
    "1. Load IOI datasets and setup the model\n",
    "2. Learn linear parameters from training data\n",
    "3. Train DAS featurizers on selected attention heads\n",
    "4. Save trained models in submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n",
      "Using device: cuda:1\n",
      "Model: gemma\n",
      "Heads to train: [(7, 6), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve()))\n",
    "\n",
    "from tasks.IOI_task.ioi_task import get_causal_model, get_counterfactual_datasets, get_token_positions\n",
    "from CausalAbstraction.experiments.attention_head_experiment import PatchAttentionHeads\n",
    "from CausalAbstraction.experiments.filter_experiment import FilterExperiment\n",
    "from CausalAbstraction.experiments.aggregate_experiments import attention_head_baselines\n",
    "from baselines.ioi_baselines.ioi_utils import (\n",
    "    log_diff, clear_memory, checker, filter_checker, custom_loss, \n",
    "    ioi_loss_and_metric_fn, setup_pipeline\n",
    ")\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Clear memory before starting\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set device\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"llama\"  # Will use google/gemma-2-2b\n",
    "heads_list = [(7, 6), (8,1)]  \n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Heads to train: {heads_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data and Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mib-bench/ioi_private_test couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/atticus/.cache/huggingface/datasets/mib-bench___ioi_private_test/default/0.0.0/3513057605230348398b751d59dfeb581c115922 (last modified on Fri May 30 21:55:12 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: dict_keys(['s1_io_flip_train', 's2_io_flip_train', 's1_ioi_flip_s2_ioi_flip_train', 's1_io_flip_test', 's2_io_flip_test', 's1_ioi_flip_s2_ioi_flip_test', 's1_io_flip_testprivate', 's2_io_flip_testprivate', 's1_ioi_flip_s2_ioi_flip_testprivate', 'same_train', 'same_test', 'same_testprivate'])\n",
      "\n",
      "Sample input:\n",
      "  Raw input: As Carl and Maria left the consulate, Carl gave a fridge to\n",
      "  Name A: Carl\n",
      "  Name B: Maria\n",
      "  Name C: Carl\n"
     ]
    }
   ],
   "source": [
    "# Get counterfactual datasets with placeholder causal model\n",
    "# We'll update the causal model with learned parameters later\n",
    "causal_model = get_causal_model({\"bias\": 0.0, \"token_coeff\": 0.0, \"position_coeff\": 0.0})\n",
    "counterfactual_datasets = get_counterfactual_datasets(hf=True, size=1000, load_private_data=True)#None)\n",
    "\n",
    "print(\"Available datasets:\", counterfactual_datasets.keys())\n",
    "\n",
    "# Get a sample to display\n",
    "sample_dataset = next(iter(counterfactual_datasets.values()))\n",
    "if len(sample_dataset) > 0:\n",
    "    sample = sample_dataset[0]\n",
    "    print(\"\\nSample input:\")\n",
    "    print(f\"  Raw input: {sample['input']['raw_input']}\")\n",
    "    print(f\"  Name A: {sample['input']['name_A']}\")\n",
    "    print(f\"  Name B: {sample['input']['name_B']}\")\n",
    "    print(f\"  Name C: {sample['input']['name_C']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157c8c3ad2e94880b2f5c259b3ba649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline device: cuda:1\n",
      "Model: Gemma2ForCausalLM\n",
      "Hidden size: 2304\n",
      "Number of layers: 26\n",
      "\n",
      "Testing model on sample:\n",
      "INPUT: As Carl and Maria left the consulate, Carl gave a fridge to\n",
      "EXPECTED OUTPUT: Maria\n",
      "MODEL PREDICTION:  Maria\n"
     ]
    }
   ],
   "source": [
    "# Set up pipeline\n",
    "pipeline, default_batch_size = setup_pipeline(model_name, device, eval_batch_size=None)\n",
    "batch_size = 128  # You can adjust this based on your GPU memory\n",
    "eval_batch_size = 1024\n",
    "\n",
    "print(f\"Pipeline device: {pipeline.model.device}\")\n",
    "print(f\"Model: {pipeline.model.__class__.__name__}\")\n",
    "print(f\"Hidden size: {pipeline.model.config.hidden_size}\")\n",
    "print(f\"Number of layers: {pipeline.get_num_layers()}\")\n",
    "\n",
    "# Test model on a sample\n",
    "if len(sample_dataset) > 0:\n",
    "    sample = sample_dataset[0]\n",
    "    print(\"\\nTesting model on sample:\")\n",
    "    print(f\"INPUT: {sample['input']['raw_input']}\")\n",
    "    expected = causal_model.run_forward(sample['input'])['raw_output']\n",
    "    print(f\"EXPECTED OUTPUT: {expected}\")\n",
    "    print(f\"MODEL PREDICTION: {pipeline.dump(pipeline.generate(sample['input']['raw_input']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Filter Datasets Based on Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering datasets based on model performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering s1_io_flip_train: 100%|██████████| 1/1 [00:05<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_io_flip_train': kept 866/1000 examples (86.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering s2_io_flip_train: 100%|██████████| 1/1 [00:05<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's2_io_flip_train': kept 855/1000 examples (85.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering s1_ioi_flip_s2_ioi_flip_train: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_ioi_flip_s2_ioi_flip_train': kept 864/1000 examples (86.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering s1_io_flip_test: 100%|██████████| 1/1 [00:05<00:00,  5.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_io_flip_test': kept 775/1000 examples (77.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering s2_io_flip_test: 100%|██████████| 1/1 [00:05<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's2_io_flip_test': kept 765/1000 examples (76.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering s1_ioi_flip_s2_ioi_flip_test: 100%|██████████| 1/1 [00:05<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_ioi_flip_s2_ioi_flip_test': kept 772/1000 examples (77.2%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering s1_io_flip_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_io_flip_testprivate': kept 880/1000 examples (88.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering s2_io_flip_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's2_io_flip_testprivate': kept 876/1000 examples (87.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering s1_ioi_flip_s2_ioi_flip_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_ioi_flip_s2_ioi_flip_testprivate': kept 880/1000 examples (88.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering same_train: 100%|██████████| 1/1 [00:05<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'same_train': kept 906/1000 examples (90.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering same_test: 100%|██████████| 1/1 [00:05<00:00,  5.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'same_test': kept 824/1000 examples (82.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering same_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'same_testprivate': kept 914/1000 examples (91.4%)\n",
      "\n",
      "Total filtering results:\n",
      "Original examples: 12000\n",
      "Kept examples: 10177\n",
      "Overall keep rate: 84.8%\n",
      "\n",
      "Token positions: ['all']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the datasets\n",
    "print(\"Filtering datasets based on model performance...\")\n",
    "exp = FilterExperiment(pipeline, causal_model, filter_checker)\n",
    "filtered_datasets = exp.filter(counterfactual_datasets, verbose=True, batch_size=eval_batch_size)\n",
    "\n",
    "# Get token positions\n",
    "token_positions = get_token_positions(pipeline, causal_model)\n",
    "print(f\"\\nToken positions: {[pos.id for pos in token_positions]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Learn Linear Parameters from Training Data\n",
    "\n",
    "IOI requires linear parameters (bias, token_coeff, position_coeff) for the causal mechanism. We'll learn these from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading linear parameters from: baselines/ioi_linear_params.json\n",
      "Using coefficients for gemma:\n",
      "  bias: 0.04835902899503708\n",
      "  token_coeff: 0.767971899360421\n",
      "  position_coeff: 2.004627879709005\n"
     ]
    }
   ],
   "source": [
    "# Load linear parameters from external file\n",
    "linear_params_file = \"baselines/ioi_linear_params.json\"\n",
    "\n",
    "print(f\"Loading linear parameters from: {linear_params_file}\")\n",
    "try:\n",
    "    if os.path.isfile(linear_params_file):\n",
    "        with open(linear_params_file, 'r') as f:\n",
    "            all_coeffs = json.load(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Linear parameters file not found: {linear_params_file}\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to load linear_params: {e}\")\n",
    "\n",
    "# Find the coefficients for this model\n",
    "if model_name in all_coeffs:\n",
    "    coeffs = all_coeffs[model_name]\n",
    "elif \"default\" in all_coeffs:\n",
    "    coeffs = all_coeffs[\"default\"]\n",
    "else:\n",
    "    # Use the first available coefficients\n",
    "    coeffs = next(iter(all_coeffs.values()))\n",
    "\n",
    "# Validate required keys\n",
    "required_keys = ['bias', 'token_coeff', 'position_coeff']\n",
    "for key in required_keys:\n",
    "    if key not in coeffs:\n",
    "        raise ValueError(f\"Missing required key '{key}' in linear_coeffs for model {model_name}\")\n",
    "\n",
    "intercept = coeffs['bias']\n",
    "token_coef = coeffs['token_coeff']\n",
    "position_coef = coeffs['position_coeff']\n",
    "\n",
    "print(f\"Using coefficients for {model_name}:\")\n",
    "print(f\"  bias: {intercept}\")\n",
    "print(f\"  token_coeff: {token_coef}\")\n",
    "print(f\"  position_coeff: {position_coef}\")\n",
    "\n",
    "# Store parameters\n",
    "linear_params = {\n",
    "    \"bias\": float(intercept),\n",
    "    \"token_coeff\": float(token_coef),\n",
    "    \"position_coeff\": float(position_coef)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Update Causal Model with Learned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal model updated with learned parameters\n"
     ]
    }
   ],
   "source": [
    "# Update the causal model with learned parameters\n",
    "causal_model = get_causal_model(linear_params)\n",
    "print(\"Causal model updated with learned parameters\")\n",
    "\n",
    "# Clear memory before training\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets: ['s1_io_flip_train', 's2_io_flip_train', 's1_ioi_flip_s2_ioi_flip_train']\n",
      "Test datasets: ['s1_io_flip_test', 's1_io_flip_testprivate', 's2_io_flip_test', 's2_io_flip_testprivate', 's1_ioi_flip_s2_ioi_flip_test', 's1_ioi_flip_s2_ioi_flip_testprivate']\n"
     ]
    }
   ],
   "source": [
    "# Setup counterfactual names for IOI\n",
    "counterfactuals = [\"s1_io_flip\", \"s2_io_flip\", \"s1_ioi_flip_s2_ioi_flip\"]\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for counterfactual in counterfactuals:\n",
    "    if \"same\" in counterfactual:\n",
    "        print(\"hi\")\n",
    "        continue\n",
    "    if counterfactual + \"_train\" in filtered_datasets:\n",
    "        train_data[counterfactual + \"_train\"] = filtered_datasets[counterfactual + \"_train\"]  # Limit to 1000 samples for training\n",
    "    if counterfactual + \"_test\" in filtered_datasets:\n",
    "        test_data[counterfactual + \"_test\"] = filtered_datasets[counterfactual + \"_test\"]\n",
    "    if counterfactual + \"_testprivate\" in filtered_datasets:\n",
    "        test_data[counterfactual + \"_testprivate\"] = filtered_datasets[counterfactual + \"_testprivate\"]\n",
    "\n",
    "print(\"Train datasets:\", list(train_data.keys()))\n",
    "print(\"Test datasets:\", list(test_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ioi_submission_results\n",
      "Models will be saved to: ioi_submission\n"
     ]
    }
   ],
   "source": [
    "# Setup experiment configuration for DAS\n",
    "config = {\n",
    "    \"evaluation_batch_size\": eval_batch_size,\n",
    "    \"batch_size\": batch_size, \n",
    "    \"training_epoch\": 2,  # Number of training epochs\n",
    "    \"check_raw\": True,\n",
    "    \"n_features\": 32,  # Feature dimension for DAS\n",
    "    \"regularization_coefficient\": 0.0, \n",
    "    \"output_scores\": True, \n",
    "    \"shuffle\": True, \n",
    "    \"temperature_schedule\": (1.0, 0.01),  # Temperature annealing for training\n",
    "    \"init_lr\": 1.0,\n",
    "    \"loss_and_metric_fn\": lambda pipeline, intervenable_model, batch, model_units_list: \n",
    "        ioi_loss_and_metric_fn(pipeline, intervenable_model, batch, model_units_list),\n",
    "}\n",
    "\n",
    "# Setup directories for saving results\n",
    "results_dir = \"ioi_submission_results\"\n",
    "model_dir = \"ioi_submission\"\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "print(f\"Results will be saved to: {results_dir}\")\n",
    "print(f\"Models will be saved to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train DAS on Output Position Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training DAS for output_position variable...\n",
      "============================================================\n",
      "Running DAS method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 21/21 [00:22<00:00,  1.09s/it, loss=155, mse=157, rmse=12.5]\n",
      "Epoch: 1: 100%|██████████| 21/21 [00:23<00:00,  1.10s/it, loss=151, mse=151, rmse=12.3]\n",
      "Epoch: 100%|██████████| 2/2 [00:46<00:00, 23.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DAS training completed for output_position\n",
      "Models saved to: ioi_submission/ioi_task_Gemma2ForCausalLM_output_position\n"
     ]
    }
   ],
   "source": [
    "# Train DAS for output_position variable\n",
    "print(\"\\nTraining DAS for output_position variable...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fix for in-place operation error during training\n",
    "# Set model to training mode and disable in-place operations\n",
    "pipeline.model.train()\n",
    "if hasattr(pipeline.model.config, 'use_cache'):\n",
    "    pipeline.model.config.use_cache = False\n",
    "\n",
    "target_variable = \"output_position\"\n",
    "position_model_dir = os.path.join(model_dir, f\"ioi_task_{pipeline.model.__class__.__name__}_{target_variable}\")\n",
    "\n",
    "attention_head_baselines(\n",
    "    pipeline=pipeline, \n",
    "    task=causal_model, \n",
    "    token_positions=token_positions, \n",
    "    train_data=train_data, \n",
    "    test_data=test_data, \n",
    "    config=config, \n",
    "    target_variables=[target_variable], \n",
    "    checker=lambda logits, params: checker(logits, params, pipeline), \n",
    "    verbose=True, \n",
    "    results_dir=results_dir,\n",
    "    model_dir=position_model_dir,\n",
    "    heads_list=heads_list,\n",
    "    skip=[\"full_vector\", \"DBM+SVD\", \"DBM+PCA\", \"DBM\", \"DBM+SAE\"]  # Only run DAS\n",
    ")\n",
    "\n",
    "print(f\"\\nDAS training completed for {target_variable}\")\n",
    "print(f\"Models saved to: {position_model_dir}\")\n",
    "\n",
    "# Clear memory\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train DAS on Output Token Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training DAS for output_token variable...\n",
      "============================================================\n",
      "Running DAS method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 21/21 [00:23<00:00,  1.11s/it, loss=116, mse=119, rmse=10.9]\n",
      "Epoch: 1: 100%|██████████| 21/21 [00:23<00:00,  1.11s/it, loss=113, mse=113, rmse=10.6]\n",
      "Epoch: 100%|██████████| 2/2 [00:46<00:00, 23.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DAS training completed for output_token\n",
      "Models saved to: ioi_submission/ioi_task_Gemma2ForCausalLM_output_token\n"
     ]
    }
   ],
   "source": [
    "# Train DAS for output_token variable\n",
    "print(\"\\nTraining DAS for output_token variable...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target_variable = \"output_token\"\n",
    "token_model_dir = os.path.join(model_dir, f\"ioi_task_{pipeline.model.__class__.__name__}_{target_variable}\")\n",
    "\n",
    "config[\"n_features\"] = 32\n",
    "\n",
    "attention_head_baselines(\n",
    "    pipeline=pipeline, \n",
    "    task=causal_model, \n",
    "    token_positions=token_positions, \n",
    "    train_data=train_data, \n",
    "    test_data=test_data, \n",
    "    config=config, \n",
    "    target_variables=[target_variable], \n",
    "    checker=lambda logits, params: checker(logits, params, pipeline), \n",
    "    verbose=True, \n",
    "    results_dir=results_dir,\n",
    "    model_dir=token_model_dir,\n",
    "    heads_list=heads_list,\n",
    "    skip=[\"full_vector\", \"DBM+SVD\", \"DBM+PCA\", \"DBM\", \"DBM+SAE\"]  # Only run DAS\n",
    ")\n",
    "\n",
    "print(f\"\\nDAS training completed for {target_variable}\")\n",
    "print(f\"Models saved to: {token_model_dir}\")\n",
    "\n",
    "# Clear memory\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Linear Parameters for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear parameters saved to: ioi_submission/ioi_linear_params.json\n",
      "{\n",
      "  \"gemma\": {\n",
      "    \"bias\": 0.04835902899503708,\n",
      "    \"token_coeff\": 0.767971899360421,\n",
      "    \"position_coeff\": 2.004627879709005\n",
      "  },\n",
      "  \"model_class\": \"Gemma2ForCausalLM\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save the linear parameters we learned\n",
    "linear_params_file = os.path.join(model_dir, \"ioi_linear_params.json\")\n",
    "\n",
    "params_to_save = {\n",
    "    model_name: linear_params,\n",
    "    \"model_class\": pipeline.model.__class__.__name__\n",
    "}\n",
    "\n",
    "with open(linear_params_file, 'w') as f:\n",
    "    json.dump(params_to_save, f, indent=2)\n",
    "\n",
    "print(f\"Linear parameters saved to: {linear_params_file}\")\n",
    "print(json.dumps(params_to_save, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Verify Saved Models\n",
    "\n",
    "Let's verify that the models were saved correctly by listing the saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model structure:\n",
      "\n",
      "ioi_submission/\n",
      "ioi_submission/\n",
      "  ioi_linear_params.json\n",
      "  ioi_task_Gemma2ForCausalLM_output_position/\n",
      "    DAS_Gemma2ForCausalLM_output_position/\n",
      "      AttentionHead(Layer:7,Head:6,Token:all)_featurizer\n",
      "      AttentionHead(Layer:7,Head:6,Token:all)_indices\n",
      "      AttentionHead(Layer:7,Head:6,Token:all)_inverse_featurizer\n",
      "      AttentionHead(Layer:8,Head:1,Token:all)_featurizer\n",
      "      AttentionHead(Layer:8,Head:1,Token:all)_indices\n",
      "      AttentionHead(Layer:8,Head:1,Token:all)_inverse_featurizer\n",
      "  ioi_task_Gemma2ForCausalLM_output_token/\n",
      "    DAS_Gemma2ForCausalLM_output_token/\n",
      "      AttentionHead(Layer:7,Head:6,Token:all)_featurizer\n",
      "      AttentionHead(Layer:7,Head:6,Token:all)_indices\n",
      "      AttentionHead(Layer:7,Head:6,Token:all)_inverse_featurizer\n",
      "      AttentionHead(Layer:8,Head:1,Token:all)_featurizer\n",
      "      AttentionHead(Layer:8,Head:1,Token:all)_indices\n",
      "      AttentionHead(Layer:8,Head:1,Token:all)_inverse_featurizer\n",
      "\n",
      "✓ Submission ready!\n",
      "\n",
      "Your submission folder 'ioi_submission' contains:\n",
      "- Trained DAS featurizers for both output_position and output_token\n",
      "- Linear parameters used for the causal model\n",
      "- All necessary files for evaluation\n"
     ]
    }
   ],
   "source": [
    "# List saved model files\n",
    "print(\"\\nSaved model structure:\")\n",
    "print(f\"\\n{model_dir}/\")\n",
    "\n",
    "for root, dirs, files in os.walk(model_dir):\n",
    "    level = root.replace(model_dir, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in sorted(files)[:10]:  # Show first 10 files\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 10:\n",
    "        print(f\"{subindent}... and {len(files) - 10} more files\")\n",
    "\n",
    "print(\"\\n✓ Submission ready!\")\n",
    "print(f\"\\nYour submission folder '{model_dir}' contains:\")\n",
    "print(\"- Trained DAS featurizers for both output_position and output_token\")\n",
    "print(\"- Linear parameters used for the causal model\")\n",
    "print(\"- All necessary files for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Load Trained Models and Run Inference\n",
    "\n",
    "This section demonstrates how to load previously trained featurizers and use them for inference on test data. This is useful for:\n",
    "\n",
    "1. **Testing trained models**: Verify that saved models work correctly\n",
    "2. **Running interventions**: Use the trained featurizers to perform causal interventions on attention heads\n",
    "3. **Evaluation**: Test model performance on held-out test data\n",
    "\n",
    "The process involves:\n",
    "- Loading the trained DAS featurizers from disk\n",
    "- Running interventions on test datasets for both output_position and output_token variables\n",
    "- Collecting results for analysis\n",
    "\n",
    "This is exactly what the evaluation system will do with your submitted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained models and running inference...\n",
      "============================================================\n",
      "\n",
      "Testing DAS method for output_position...\n",
      "Loading featurizers from: ioi_submission/ioi_task_Gemma2ForCausalLM_output_position/DAS_Gemma2ForCausalLM_output_position\n",
      "Running interventions for s1_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DAS method for output_token...\n",
      "Loading featurizers from: ioi_submission/ioi_task_Gemma2ForCausalLM_output_token/DAS_Gemma2ForCausalLM_output_token\n",
      "Running interventions for s1_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer:7,Head:6,Token:all)'), AtomicModelUnit(id='AttentionHead(Layer:8,Head:1,Token:all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Inference completed!\n",
      "Results saved to: ioi_submission_results_loaded\n"
     ]
    }
   ],
   "source": [
    "# Load saved models and run inference\n",
    "# This demonstrates how to load previously trained featurizers and run interventions\n",
    "\n",
    "from CausalAbstraction.experiments.attention_head_experiment import PatchAttentionHeads\n",
    "\n",
    "print(\"Loading trained models and running inference...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Directory for saving inference results\n",
    "inference_results_dir = results_dir + \"_loaded\"\n",
    "if not os.path.exists(inference_results_dir):\n",
    "    os.makedirs(inference_results_dir)\n",
    "\n",
    "# Test both target variables\n",
    "target_variables_to_test = [\"output_position\", \"output_token\"]\n",
    "\n",
    "for target_variable in target_variables_to_test:\n",
    "    print(f\"\\nTesting DAS method for {target_variable}...\")\n",
    "    \n",
    "    config[\"n_features\"] = 32\n",
    "    \n",
    "    # Create experiment with same configuration\n",
    "    config[\"method_name\"] = \"DAS\"\n",
    "    experiment = PatchAttentionHeads(\n",
    "        pipeline=pipeline,\n",
    "        causal_model=causal_model,\n",
    "        layer_head_list=heads_list,\n",
    "        token_positions=token_positions,\n",
    "        checker=lambda logits, params: checker(logits, params, pipeline),\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Load the trained featurizers\n",
    "    method_model_dir = os.path.join(\n",
    "        model_dir, \n",
    "        f\"ioi_task_{pipeline.model.__class__.__name__}_{target_variable}\",\n",
    "        f\"DAS_{pipeline.model.__class__.__name__}_{target_variable}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading featurizers from: {method_model_dir}\")\n",
    "    experiment.load_featurizers(method_model_dir)\n",
    "    \n",
    "    # Run interventions on test data\n",
    "    raw_results = experiment.perform_interventions(\n",
    "        test_data, \n",
    "        verbose=True, \n",
    "        target_variables_list=[[target_variable]], \n",
    "        save_dir=inference_results_dir\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    del experiment, raw_results\n",
    "    clear_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Inference completed!\")\n",
    "print(f\"Results saved to: {inference_results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
