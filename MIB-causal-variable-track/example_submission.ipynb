{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MCQA Example Submission - DAS Training\n",
    "\n",
    "This notebook demonstrates how to train DAS (Direct Attribution with Subspace) featurizers for the Simple MCQA (Multiple Choice Question Answering) task using a Gemma model.\n",
    "\n",
    "## Overview\n",
    "1. Load Simple MCQA datasets and setup the model\n",
    "2. Filter datasets based on model performance  \n",
    "3. Configure experiment settings for DAS training\n",
    "4. Train DAS featurizers on residual stream representations\n",
    "5. Load and test trained models\n",
    "\n",
    "The Simple MCQA task involves answering multiple choice questions where the model needs to identify the correct answer from 4 options (A, B, C, D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "Load the Simple MCQA task components and initialize the model pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mib-bench/copycolors_mcqa_private_test couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration '4_answer_choices' at /home/atticus/.cache/huggingface/datasets/mib-bench___copycolors_mcqa_private_test/4_answer_choices/0.0.0/da600e08a8c9fe40917ac887eda693de57b9f04d (last modified on Tue Jun  3 19:36:15 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: dict_keys(['answerPosition_train', 'randomLetter_train', 'answerPosition_randomLetter_train', 'answerPosition_validation', 'randomLetter_validation', 'answerPosition_randomLetter_validation', 'answerPosition_test', 'randomLetter_test', 'answerPosition_randomLetter_test', 'answerPosition_testprivate', 'randomLetter_testprivate', 'answerPosition_randomLetter_testprivate'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972e9efef25f46488b8a61b46bc2d498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "INPUT: {'choice0': 'red', 'choice1': 'orange', 'choice2': 'brown', 'choice3': 'purple', 'question': ['brown', 'question: coconuts'], 'raw_input': 'Question: Coconuts are brown. What color are coconuts?\\nA. red\\nB. orange\\nC. brown\\nD. purple\\nAnswer:', 'symbol0': 'A', 'symbol1': 'B', 'symbol2': 'C', 'symbol3': 'D'}\n",
      "EXPECTED OUTPUT:  C\n",
      "MODEL PREDICTION:  C\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "from tasks.simple_MCQA.simple_MCQA import (\n",
    "    get_token_positions,\n",
    "    get_counterfactual_datasets,\n",
    "    get_causal_model,\n",
    ")\n",
    "from experiments.aggregate_experiments import residual_stream_baselines\n",
    "from neural.pipeline import LMPipeline\n",
    "from experiments.filter_experiment import FilterExperiment\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Get counterfactual datasets and causal model\n",
    "counterfactual_datasets = get_counterfactual_datasets(\n",
    "    hf=True, size=None, load_private_data=True\n",
    ")\n",
    "causal_model = get_causal_model()\n",
    "\n",
    "# Print available datasets\n",
    "print(\"Available datasets:\", counterfactual_datasets.keys())\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Clear Python garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    # Clear CUDA cache if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Force a synchronization point to ensure memory is freed\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "def checker(output_text, expected):\n",
    "    return expected in output_text\n",
    "\n",
    "\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "pipeline = LMPipeline(model_name, max_new_tokens=1, device=device, dtype=torch.float16)\n",
    "pipeline.tokenizer.padding_side = \"left\"\n",
    "print(\"DEVICE:\", pipeline.model.device)\n",
    "\n",
    "# Get a sample input and check model's prediction\n",
    "sampled_example = next(iter(counterfactual_datasets.values()))[0]\n",
    "print(\"INPUT:\", sampled_example[\"input\"])\n",
    "print(\n",
    "    \"EXPECTED OUTPUT:\", causal_model.run_forward(sampled_example[\"input\"])[\"raw_output\"]\n",
    ")\n",
    "print(\"MODEL PREDICTION:\", pipeline.dump(pipeline.generate(sampled_example[\"input\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Filter Datasets Based on Model Performance\n",
    "\n",
    "Filter datasets to keep only examples where the model produces correct outputs. This ensures we train featurizers on cases where the model actually succeeds at the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering datasets based on model performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering answerPosition_train: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'answerPosition_train': kept 106/110 examples (96.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering randomLetter_train: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'randomLetter_train': kept 75/110 examples (68.2%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering answerPosition_randomLetter_train: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'answerPosition_randomLetter_train': kept 71/110 examples (64.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering answerPosition_validation: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'answerPosition_validation': kept 50/50 examples (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering randomLetter_validation: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'randomLetter_validation': kept 37/50 examples (74.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering answerPosition_randomLetter_validation: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'answerPosition_randomLetter_validation': kept 35/50 examples (70.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering answerPosition_test: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'answerPosition_test': kept 50/50 examples (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering randomLetter_test: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'randomLetter_test': kept 34/50 examples (68.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering answerPosition_randomLetter_test: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'answerPosition_randomLetter_test': kept 37/50 examples (74.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering answerPosition_testprivate: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'answerPosition_testprivate': kept 50/50 examples (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering randomLetter_testprivate: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'randomLetter_testprivate': kept 34/50 examples (68.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering answerPosition_randomLetter_testprivate: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'answerPosition_randomLetter_testprivate': kept 32/50 examples (64.0%)\n",
      "\n",
      "Total filtering results:\n",
      "Original examples: 780\n",
      "Kept examples: 611\n",
      "Overall keep rate: 78.3%\n",
      "\n",
      "Token positions highlighted in samples:\n",
      "<bos>Question: Coconuts are brown. What color are coconuts?\n",
      "A. red\n",
      "B. orange\n",
      "C. purple\n",
      "**D**. brown\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the datasets based on model performance\n",
    "print(\"\\nFiltering datasets based on model performance...\")\n",
    "exp = FilterExperiment(pipeline, causal_model, checker)\n",
    "filtered_datasets = exp.filter(counterfactual_datasets, verbose=True, batch_size=1024)\n",
    "\n",
    "token_positions = get_token_positions(pipeline, causal_model)\n",
    "\n",
    "# Display token highlighting for a sample\n",
    "print(\"\\nToken positions highlighted in samples:\")\n",
    "for dataset in filtered_datasets.values():\n",
    "    for token_position in token_positions:\n",
    "        example = dataset[0]\n",
    "        print(\n",
    "            token_position.highlight_selected_token(example[\"counterfactual_inputs\"][0])\n",
    "        )\n",
    "        break\n",
    "    break\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Experiment Settings for DAS Training\n",
    "\n",
    "Set up the training configuration including batch sizes, training epochs, feature dimensions, and target variables. For Simple MCQA, we focus on the `answer_pointer` variable which represents the model's ability to point to the correct answer choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "\n",
    "# Use original config for all models\n",
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"evaluation_batch_size\": 1024,\n",
    "    \"training_epoch\": 1,\n",
    "    \"n_features\": 16,\n",
    "    \"regularization_coefficient\": 0.0,\n",
    "    \"output_scores\": False,\n",
    "}\n",
    "\n",
    "names = [\"answerPosition\", \"randomLetter\", \"answerPosition_randomLetter\"]\n",
    "\n",
    "# Prepare train and test data dictionaries\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for name in names:\n",
    "    if name + \"_train\" in filtered_datasets:\n",
    "        train_data[name + \"_train\"] = filtered_datasets[name + \"_train\"]\n",
    "    if name + \"_test\" in filtered_datasets:\n",
    "        test_data[name + \"_test\"] = filtered_datasets[name + \"_test\"]\n",
    "    # Uncomment the line below if testprivate datasets are available\n",
    "    if name + \"_testprivate\" in filtered_datasets:\n",
    "        test_data[name + \"_testprivate\"] = filtered_datasets[name + \"_testprivate\"]\n",
    "\n",
    "verbose = False\n",
    "results_dir = \"mock_submission_results\"\n",
    "model_dir = os.path.join(\n",
    "    \"mock_submission\", \"4_answer_MCQA_Gemma2ForCausalLM_answer_pointer\"\n",
    ")\n",
    "target_variables = [\"answer_pointer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train DAS Featurizers on Residual Stream\n",
    "\n",
    "Train DAS (Direct Attribution with Subspace) featurizers on the residual stream representations. DAS learns a low-dimensional subspace that captures the relevant features for the causal variable we want to model.\n",
    "\n",
    "The training process:\n",
    "1. Extract residual stream activations from the specified layers\n",
    "2. Learn a featurizer that maps high-dimensional activations to a lower-dimensional feature space\n",
    "3. Train the feature space to predict the target causal variable\n",
    "4. Save the trained featurizers for later use in interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DAS method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s, loss=1.68, accuracy=0.57, token_accuracy=0.57]\n",
      "Epoch: 100%|██████████| 1/1 [00:03<00:00,  3.57s/it]\n",
      "Epoch: 0: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s, loss=2.17, accuracy=0.2, token_accuracy=0.2]  \n",
      "Epoch: 100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\n",
      "Epoch: 0: 100%|██████████| 4/4 [00:02<00:00,  1.48it/s, loss=2.2, accuracy=0.27, token_accuracy=0.27] \n",
      "Epoch: 100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run DAS method (Direct Attribution with Subspace)\n",
    "print(\"Running DAS method...\")\n",
    "\n",
    "residual_stream_baselines(\n",
    "    pipeline=pipeline,\n",
    "    task=causal_model,\n",
    "    token_positions=token_positions,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    config=config,\n",
    "    target_variables=target_variables,\n",
    "    checker=checker,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    verbose=verbose,\n",
    "    model_dir=model_dir,\n",
    "    results_dir=results_dir,\n",
    "    methods=[\"DAS\"],  # Only run DAS method\n",
    ")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Methods (Optional)\n",
    "\n",
    "The following cells demonstrate how to train other baseline methods like DBM (Distributed Basis Models) and DBM+SAE (with Sparse Autoencoders). These are commented out but can be uncommented to experiment with different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run DBM+SAE method (uncomment to use)\n",
    "# NOTE: This method requires the sae_lens library and is specific to certain models\n",
    "\n",
    "# print(\"Running DBM+SAE method...\")\n",
    "\n",
    "# residual_stream_baselines(\n",
    "#     pipeline=pipeline,\n",
    "#     task=causal_model,\n",
    "#     token_positions=token_positions,\n",
    "#     train_data=train_data,\n",
    "#     test_data=test_data,\n",
    "#     config=config,\n",
    "#     target_variables=target_variables,\n",
    "#     checker=checker,\n",
    "#     start=start,\n",
    "#     end=end,\n",
    "#     verbose=verbose,\n",
    "#     model_dir=model_dir,\n",
    "#     results_dir=results_dir,\n",
    "#     methods=[\"DBM+SAE\"]  # Only run DBM+SAE method\n",
    "# )\n",
    "\n",
    "# clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run DBM method (uncomment to use)\n",
    "# print(\"Running DBM method...\")\n",
    "\n",
    "# residual_stream_baselines(\n",
    "#     pipeline=pipeline,\n",
    "#     task=causal_model,\n",
    "#     token_positions=token_positions,\n",
    "#     train_data=train_data,\n",
    "#     test_data=test_data,\n",
    "#     config=config,\n",
    "#     target_variables=target_variables,\n",
    "#     checker=checker,\n",
    "#     start=start,\n",
    "#     end=end,\n",
    "#     verbose=verbose,\n",
    "#     model_dir=model_dir,\n",
    "#     results_dir=results_dir,\n",
    "#     methods=[\"DBM\"]  # Only run DBM method\n",
    "# )\n",
    "\n",
    "# clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Trained Models and Run Inference\n",
    "\n",
    "This section demonstrates how to load previously trained featurizers and use them for inference on test data. This is useful for:\n",
    "\n",
    "1. **Testing trained models**: Verify that saved models work correctly\n",
    "2. **Running interventions**: Use the trained featurizers to perform causal interventions\n",
    "3. **Evaluation**: Test model performance on held-out test data\n",
    "\n",
    "The process involves:\n",
    "- Loading the trained featurizer from disk\n",
    "- Running interventions on test datasets  \n",
    "- Collecting results for analysis\n",
    "\n",
    "This is exactly what the evaluation system will do with your submitted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained models and running inference...\n",
      "Testing DAS method...\n",
      "Inference completed!\n"
     ]
    }
   ],
   "source": [
    "# Example: Load saved models and run inference\n",
    "# This demonstrates how to load previously trained featurizers and run interventions\n",
    "\n",
    "from experiments.residual_stream_experiment import PatchResidualStream\n",
    "\n",
    "print(\"Loading trained models and running inference...\")\n",
    "\n",
    "for method in [\"DAS\"]:  # Can also test \"DBM\", \"DBM+SAE\", etc.\n",
    "    print(f\"Testing {method} method...\")\n",
    "\n",
    "    # Create experiment with same configuration\n",
    "    config[\"method_name\"] = method\n",
    "    experiment = PatchResidualStream(\n",
    "        pipeline,\n",
    "        causal_model,\n",
    "        list(range(start, end)),\n",
    "        token_positions,\n",
    "        checker,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Set up SAE loader if needed for DBM+SAE method\n",
    "    if method == \"DBM+SAE\":\n",
    "        from sae_lens import SAE\n",
    "\n",
    "        def sae_loader(layer):\n",
    "            sae, _, _ = SAE.from_pretrained(\n",
    "                release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "                sae_id=f\"layer_{layer}/width_16k/canonical\",\n",
    "                device=\"cpu\",\n",
    "            )\n",
    "            return sae\n",
    "\n",
    "        experiment.build_SAE_feature_intervention(sae_loader)\n",
    "\n",
    "    # Load the trained featurizers\n",
    "    method_model_dir = os.path.join(\n",
    "        model_dir,\n",
    "        f\"{method}_{pipeline.model.__class__.__name__}_{'-'.join(target_variables)}\",\n",
    "    )\n",
    "\n",
    "    experiment.load_featurizers(method_model_dir)\n",
    "\n",
    "    # Run interventions on test data\n",
    "    raw_results = experiment.perform_interventions(\n",
    "        test_data,\n",
    "        verbose=verbose,\n",
    "        target_variables_list=[target_variables],\n",
    "        save_dir=results_dir + \"_loaded\",\n",
    "    )\n",
    "\n",
    "    # Clean up\n",
    "    del experiment, raw_results\n",
    "    clear_memory()\n",
    "\n",
    "print(\"Inference completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
