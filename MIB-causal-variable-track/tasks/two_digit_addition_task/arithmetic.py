import sys, os
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

from CausalAbstraction.causal.causal_model import CausalModel
from CausalAbstraction.neural.LM_units import TokenPosition, get_last_token_index

from tasks.hf_dataloader import load_hf_dataset

from copy import deepcopy
import re
import random

def get_causal_model():
    variables = [
        "raw_input",      # Required by CausalModel
        "op1_tens", "op1_ones",
        "op2_tens", "op2_ones", 
        "ones_carry",
        "hundreds_out", "tens_out", "ones_out",
        "raw_output"      # Required by CausalModel
    ]

    # Allowed values for each variable.
    values = {
        "raw_input": [""],  # Placeholder, actual values generated by mechanism
        "op1_tens": list(range(10)),
        "op1_ones": list(range(10)),
        "op2_tens": list(range(10)),
        "op2_ones": list(range(10)),
        "ones_carry": [0, 1],
        "ones_out": list(range(10)),
        "tens_out": list(range(10)),
        "hundreds_out": [0, 1],
        "raw_output": [""]  # Placeholder, actual values generated by mechanism
    }

    # Specify parent relationships for each node.
    parents = {
        "raw_input": ["op1_tens", "op1_ones", "op2_tens", "op2_ones"],  # Depends on the operands
        "op1_tens": [],
        "op1_ones": [],
        "op2_tens": [],
        "op2_ones": [],
        "ones_carry": ["op1_ones", "op2_ones"],
        "ones_out": ["op1_ones", "op2_ones"],
        "tens_out": ["op1_tens", "op2_tens", "ones_carry"],
        "hundreds_out": ["op1_tens", "op2_tens", "ones_carry"],
        "raw_output": ["hundreds_out", "tens_out", "ones_out"]  # Depends on the result digits
    }

    # Define the mechanisms (the functions computing each node's value).
    mechanisms = {
        # Generate the raw input based on operands
        "raw_input": lambda op1_tens, op1_ones, op2_tens, op2_ones: 
            f"Q: How much is {op1_tens}{op1_ones} plus {op2_tens}{op2_ones}? A: ",
        
        # Base input nodes: randomly choose a digit
        "op1_tens": lambda: random.choice(list(range(10))),
        "op1_ones": lambda: random.choice(list(range(10))),
        "op2_tens": lambda: random.choice(list(range(10))),
        "op2_ones": lambda: random.choice(list(range(10))),
        
        # Compute carries and outputs
        "ones_carry": lambda op1_ones, op2_ones: 1 if op1_ones + op2_ones > 9 else 0,
        "ones_out": lambda op1_ones, op2_ones: (op1_ones + op2_ones) % 10,
        "tens_out": lambda op1_tens, op2_tens, ones_carry: (op1_tens + op2_tens + ones_carry) % 10,
        "hundreds_out": lambda op1_tens, op2_tens, ones_carry: 1 if op1_tens + op2_tens + ones_carry > 9 else 0,
        
        # Generate the raw output based on result digits
        "raw_output": lambda hundreds_out, tens_out, ones_out: 
            f"{hundreds_out:01d}{tens_out:01d}{ones_out:01d}"
    }

    return CausalModel(variables, values, parents, mechanisms, id="arithmetic")


def get_counterfactual_datasets(hf=True, size=None, load_private_data=False):
    """
    Load and return counterfactual datasets for arithmetic task.
    """
    if hf:
        # Load dataset from HuggingFace with customized parsing
        datasets = {}
        for split in ["train", "test"]:
            temp = load_hf_dataset(
                dataset_path="mib-bench/arithmetic_addition",
                split=split,
                parse_fn=parse_arithmetic_example,
                size=size,
                filter_fn=lambda example: example["num_digit"] == 2,
                ignore_names=["ones_op1", "ones_op2", "tens_op1", "tens_op2", "tens_carry"],
                shuffle=True
            )
            datasets.update(temp)

        if load_private_data:
            private = load_hf_dataset(
                dataset_path="mib-bench/arithmetic_addition_private_test",
                split="test",
                parse_fn=parse_arithmetic_example,
                size=size,
                filter_fn=lambda example: example["num_digit"] == 2,
                ignore_names=["ones_op1", "ones_op2", "tens_op1", "tens_op2", "tens_carry"],
                shuffle=True
            )
            datasets.update({k+"private":v for k,v in private.items()})

        return datasets
    
    # Non-HF implementation would go here if needed
    return {}


def parse_arithmetic_example(row):
    """
    Customized parsing function for the arithmetic task.
    Returns a variables dict compatible with the causal model.
    """
    # Get the prompt string
    prompt_str = row.get("prompt", "")
    
    # Parse the prompt to extract operands
    matches = re.findall(r"\d+", prompt_str)
    if len(matches) < 2:
        raise ValueError(f"Prompt must contain at least two numbers: {prompt_str}")
    
    # Take the last two numbers as operands
    op1_str, op2_str = matches[-2], matches[-1]
    
    # Parse into tens and ones
    op1_tens = int(op1_str[-2]) if len(op1_str) > 1 else 0
    op1_ones = int(op1_str[-1])
    op2_tens = int(op2_str[-2]) if len(op2_str) > 1 else 0
    op2_ones = int(op2_str[-1])
    
    # Return variables dict (not tuple)
    return {
        "raw_input": prompt_str,
        "op1_tens": op1_tens,
        "op1_ones": op1_ones,
        "op2_tens": op2_tens,
        "op2_ones": op2_ones
    }


def get_token_positions(pipeline, causal_model):
    """
    Get token positions for arithmetic task interventions.
    """
    def get_op2_last_token_index(input_dict, pipeline):
        """
        Find the index of the last token of the second operand.
        """
        # Extract prompt from input dict
        prompt = input_dict["raw_input"] if isinstance(input_dict, dict) else input_dict
        
        matches = list(re.finditer(r"\b\d+\b", prompt))
        if len(matches) < 2:
            raise ValueError(f"Prompt must contain at least two numbers: {prompt}")
        
        op2_match = matches[-1]  # Last match
        
        # Get the substring up to the op2 match end
        substring = prompt[:op2_match.end()]
        tokenized_substring = list(pipeline.load(substring)["input_ids"][0])
        
        # The last token of op2 will be at the end of the substring
        return [len(tokenized_substring) - 1]

    token_positions = [ 
        TokenPosition(lambda x: get_op2_last_token_index(x, pipeline), pipeline, id="op2_last"),
        TokenPosition(lambda x: get_last_token_index(x, pipeline), pipeline, id="last")
    ]

    return token_positions